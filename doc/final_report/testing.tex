%%------------------------------------------------------
%  
%  Testing include for dissertation
%
%------------------------------------------------------

\section{ Unit Testing }

At the beginning of the project, Unit tests for Patridge's server and paper
preprocessing components were written in order to test that the main bulk of
the application worked before it was deployed to the production system at
\url{http://farnsworth.papro.org.uk}. These tests were written and maintained
throughout the project and occasionally had to be changed as code was
refactored. As each new feature was added to the system, the whole suite of
tests was executed to provide integration coverage and make sure that new
features did not break existing code. 

\subsection{ Back-end \& Infrastructure Testing}

The backend of the system was tested using Python unit tests based upon the
built in \emph{unittest} module and a third party testing library called
\emph{Nose} which provided automated test discovery. This meant that a user who
ran the command \emph{nose} from the project directory could automatically find
and run all of the unit tests in the project without having to set up
boilerplate test suites as in \emph{JUnit}.

In cases where Partridge calls remote services, reuses code that has already
been tested, or calls code that runs heavy computation, it can be inconvenient
or unhelpful to run unit tests that trigger these behaviours. To save time,
\emph{Mock}\cite{mock2013} was used for verifying these behaviours were being
called correctly instead.  \emph{Mock} allows the arbitrary replacement of
Python routines and objects with `mock' objects that mimic the behaviour of the
original routine and store information on how they were called or used. This
meant that calls to long-running routines and functions that already have tests
can be validated and a fake return value sent back to the caller, without
having to actually execute the code. This technique also allows the isolation
of problems within the caller code and its commmunication with the target
functions and will pass even if the target code is broken provided that the
caller code is correct\cite{saff2004mock}. The Mock testing technique was used
heavily in the paper processor which calls SAPIENTA and PDFX and also wraps
several data classes which have their own unit tests.

Partridge's web views were tested using \emph{Flask}'s in-built test client.
This is an object attached to the \emph{Flask} app that can be used to emulate
a call to a particular view\cite{flask2012}. Cookies and form information can also be
manipulated through this client, enabling comprehensive testing of each of the backends
for Partridge's query, paper upload and bookmarklet capabilities.


\subsection{ Selenium \& Front-end Testing }

The frontend parts of Partridge that were written in Javascript and HTML5 also
had to be tested to verify that the query and upload forms behaved as expected.
Selenium\cite{seleniumide} was selected to facilitate these tests.

Selenium is a browser-based unit testing framework that allows developers to
script actions that a user might undertake whilst browsing a website. Tests for
selenium can be written in Firefox using the Selenium IDE framework. These
tests can then be exported as HTML files with embedded javascript instructions
and executed in any other browser to test compatibility with the product under
test.

During the development of Partridge, several suites of tests were written using
Selenium to verify that the query form, paper upload form and bookmarklet code
behave as expected. Not only do these tests verify the behaviour of the HTML
and javascript interface, but they also helped to check the higher level
functionality of Partridge. For example, the test suite for the Partridge query
form also helps to verify that query backend is working correctly. The Selenium
tests were executed as part of the integration process when a new feature was
added to Partridge.


\section{ User Testing } 

Raymond(1999) suggests that as the developer of an open source project,
``Treating your users as co-developers is your least-hassle route to rapid code
improvement and effective debugging\cite{raymond1999cathedral}." Partridge
hasn't been developed using an open methodology since, as a dissertation
project, it has been necessary to avoid complications around work ownership.
However, Raymond's notion that software should be released early and often and
tested on its user-base is an effective one and has been widely adopted by both
open and closed source developers\cite{linux2013}\cite{unity2013}.

As soon as Iteration 1 was complete, Partridge was made open for user testing
and the primary Patridge instance (\url{http://farnsworth.papro.org.uk/}) was
advertised using several social media streams. It was made clear to any early
adopting users that the system was still in development and they were
encouraged to submit bug reports if they experienced any problems whilst using
the site. This was a great success and a number of bugs and suggested
improvements were raised both via the GitHub repository\cite{softlybug2013} and
in conversation, helping to improve Partridge and find some defects that had
previously been overlooked.


\section{ Evaluating Learning Algorithms }

As discussed above, Patridge uses a supervised learning algorithm, a Random
Decision Forest, to classify the type of each of the scientific papers added to
its database. Unlike traditional procedural algorithms that can be unit tested
and in some cases formally proven to behave uniformly at
runtime\cite{filliatre2007formal}, supervised learning algorithms are imperfect
systems that build `working models' from known data. This makes the performance
of supervised learning systems much more difficult to assess. Russell and
Norvig(2010) suggest that ``a learning system is good if it produces hypotheses
that do a good job of predicting the classification of unseen
examples\cite{russell2010artificial}." It is suggested that when training a
supervised learning system, some of the example data should be kept back when
during the training phase and instead, used as `unseen examples' and classified
by the newly trained system to judge its accuracy
\cite{alpaydin2004introduction}\cite{russell2010artificial}.

A common technique for partitioning data into `seen' and `unseen' or training
and test data sets is to simply split the data into two sets through the use of
random sampling.  A 60:40 divide between training and test data is common.
Witten explains that ``it is better to use more than half the data for
training, even at the expense of test data\cite{witten2005data}." 

\subsection{ Three Fold Validation }

Three fold validation is a commonly used method for using all of the available
data for both testing and training whilst still maintaining . As the name
suggests, test data is split into three disjoint sets and then the classifier
is trained on two of the sets and tested with the third. This is repeated three
times so that all sets are used for both training and testing. Results from
each of the folds can be represented separately or an average taken.

\subsection{ Evaluation Result Analysis } 

Results from the evaluation methods discussed above can be presented in a
number of ways. When working with machine learning systems, it is important to
analyse performance using a selection of techniques in order to get a
comprehensive image of the capabilities and restrictions of the system.

Classifier Accuracy (CA), Receiver Operating Characteristics
(ROC) charts and Area Under the Curve (AUC) give a general overview of the
classifier's behaviour, whilst confusion matrices, recall, precision and
f-measure give a detailed view of which specific data types were most
frequently misclassified and what they were mistaken for. 

\subsection{ Classifier Accuracy } 

The CA of a supervised classifier is simply represented as the percentage of
the test data that the classifier was able to predict correctly. CA values that
tend towards 1.0 correspond to a more accurate classifier. In a Three Fold
validation, the CA for the classifier is calculated by taking the mean of the
individual performance values from each of the test/train cycles. 

3 Fold Validation on the Partridge corpus  

\subsection{ Receiver Operating Characteristics Chart}

Receiver Operating Characteristics (ROC) Charts are used to visually represent
the trade-off between True Positives(TP) and False Positives(FP) as identified
by the classifier under evaluation. Witten(2005) explains that ROC is a method
reappropriated from signal processing analysts who use it to visualise ``...hit
rate and false alarm rate over noisy communication
channels\cite{witten2005data}." The ratio of TP to FP classifications is
measured with increasing numbers of test samples from left to right. 

ROC charts are often divided into halves diagonally by a line indicating where
the TP-FP ratio is equal. There may be multiple plots on the axes if several
learning algorithms are under evaluation. A graph plot that presents a steep
curve that mostly fits into the upper left half of the graph shows a high TP-FP
ratio which is a sign that the classifier is working well.  Figure
\ref{fig:roc_random_forest} shows the ROC Chart for Partridge's Topic
Classifier after training on 60\% of the Partridge corpus and testing on 40\%.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.65\textwidth]{images/testing/ROC.png}
\caption{ ROC Curve for Partridge's Random Forest Topic Classifier}
\label{fig:roc_random_forest}
\end{center}
\end{figure}

\subsection{ Area Under Curve }

The Area Under the Curve (AUC) measure is calculated as a percentage of  the
area underneath a ROC curve. This can again be used to give an indication of
the trade-off between true positive and false positive classification. It is a
percentage measurement, where 100\% indicates a high TP-FP ratio. AUC is often
used instead of visualising a ROC graph for brevity or where average
performance of a classifier over several train-test cycles is to be calculated.
The average AUC for Partridge's Random Tree Paper Type classifier after is
96.6\% after three fold validation is executed over the whole set of papers.

Comparison of this result with the above graph does show discrepancies between
simple 60:40 proportional validation and 3-fold validation - the average
performance of the latter seems to be slightly more accurate. This may be
because a slightly higher number of training examples (66\% rather than 60\%)
were used to train the classifiers during 3-fold validation and that the errors
were averaged out at the end of the process.

\subsection{ Confusion Matrices}

Confusion Matrices are a way of showing comprehensively which classes a machine
learning system struggles to discriminate between the most. Known classes are
shown on the vertical axis of the matrix and predicted classes are shown across
the horizontal. This allows inference of which data classes a trained
classifier struggles to label most frequently and any classes that it commonly
confuses them with. A set of confusion matrices for the Paper Type Classifier
in Partridge can be seen in Figure \ref{fig:conf_matrices} below.

\begin{figure}[!h]

\centering

\begin{subfigure}[b]{\textwidth}

\caption{Fold 1 Results}
\centering
\begin{tabular}{| l || l | l | l |}
\hline
	&	Case Study&	Research&	Review\\
\hline
\hline
Case Study&	83&		2&		12\\
\hline
Research&	3&		255&		0\\
\hline
Review&		4&		2&		159\\
\hline

\end{tabular}
\end{subfigure}


\begin{subfigure}[b]{\textwidth}

\caption{Fold 2 Results}
\centering
\begin{tabular}{| l || l | l | l |}
\hline
&	        Case Study&     Research&	Review\\
\hline
\hline
Case Study&     85    &         2&		10\\
\hline
Research&	0   &		256&		2\\
\hline
Review&		5&		1&		159\\
\hline
\end{tabular}

\end{subfigure}

\begin{subfigure}[b]{\textwidth}

\caption{Fold 3 Results}
\centering
\begin{tabular}{| l || l | l | l |}
\hline
&		Case Study&	Research&	Review\\
\hline
\hline
Case Study&	82&		3	&	12 \\
Research&	0&		257	&	1\\
Review&		7&		2	&	156\\
\hline
\end{tabular}

\end{subfigure}

\caption{ Confusion matricies for the 3-Fold validation process}
\label{fig:conf_matrices}

\end{figure}

These results show that Partridge's decision tree is very effective at
classifying the three document types in its database. It is also clear however,
that whilst the classifier is outstanding at recognising Research papers, it
sometimes struggles to discriminate between Case Study and review papers. This
may be because case studies can be considered a specific type of review paper
and the two classes share more similar CoreSC features than they do with
research papers.  

\subsection{ Precision, Recall and F-Measure }

From a classifier's confusion matrices, it is possible to calculate its
Precision, Recall and F-Measure values. These can be used to make further
assertions about the validity of a classifier's behaviour
\cite{witten2005data}.

The Precision of a classifier is the fraction of data samples retrieved that
were correctly classified. This is defined as:
\[Precision = \frac{tp}{tp+fp} \]

The Recall of a classifier is the proportion of samples within a given class
that the classifier is able to correctly classify. For example: the number of
apples that a classifier identifies as apples within a training set of fruit.
This is defined as:
\[Recall = \frac{tp}{tp+fn} \]

The F-Measure of a classifier is used to characterise both Recall and Precision
in a single value. The F-Measure of a classifer can be defined as:
\[
F-Measure = \frac{ 2 \times Recall \times Precision} { Recall + Precision } = 
\frac{2 . TP}{ 2 . TP + FP + FN }
\]


\begin{figure}[!th]

\centering
\begin{tabular}{| l | l | l | l | l |}
\hline
&        &\textbf{Recall}&\textbf{Precision}&\textbf{F-Measure}\\
\hline
\hline
\multirow{4}{*}{Fold 1} & Case Study & 0.85 & 0.92 & 0.89 \\
                        & Review     & 0.96 & 0.92 & 0.94 \\
                        & Research   & 0.98 & 0.98 & 0.98 \\
\hline

\multirow{4}{*}{Fold 2} & Case Study & 0.87 & 0.94 & 0.91 \\
                        & Review     & 0.96 & 0.92 & 0.94 \\
                        & Research   & 0.99 & 0.98 & 0.99 \\

\hline

\multirow{4}{*}{Fold 3} &Case Study & 0.84 & 0.92 & 0.88 \\
                        & Review    & 0.95 & 0.92 & 0.93 \\
                        & Research  & 0.99 & 0.98 & 0.99 \\


\hline


\end{tabular}

\caption{Recall Precision and F-Measure values for Partridge Topic Classifier}
\label{fig:fmeasure_table}

\end{figure}

Recall, Precision and F-Measure was calculated for each of the classes and in
each iteration of the three fold validation sequence. The results in Figure
\ref{fig:fmeasure_table} show good recall, precision and f-measure for all
three document types within Partridge. These results verify that Case Study is
the hardest document type to classify as discussed above.

